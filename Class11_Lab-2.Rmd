---
title: "SMML Class 11 Lab"
author: "John Kubale"
date: "11/12/2024"
output:  pdf_document
number_sections: yes
fontsize: 12pt
---


```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo=TRUE,cache=TRUE, 
                      autodep=TRUE, cache.comments=FALSE,
                      message=FALSE, warning=FALSE,
                      fig.width=4.5, fig.height=3)
```

  
## 1. Problems with predictors 

### 1.A. Let's use cheddar data from faraway

* Description: In a study of cheddar cheese from the LaTrobe Valley of Victoria, Australia, samples of cheese were analyzed for their chemical composition and were subjected to taste tests. Overall taste scores were obtained by combining the scores from several tasters.

* Format: A data frame with 30 observations on the following 4 variables.
  + \texttt{taste}: a subjective taste score
  + \texttt{Acetic}: concentration of acetic acid (log scale)
  + \texttt{H2S}: concentration of hydrogen sulfice (log scale)
  + \texttt{Lactic}: concentration of lactic acid
  
* We will use \texttt{taste} as the outcome variable and the rest as predictors.

* On purpose, 
  + we will transform \texttt{H2S} back to the original scale; and
  + we will create another predictor \texttt{X4} as a function of \texttt{Lactic} and a random error. I.e., \texttt{X4} and \texttt{Lactic} are highly correlated (collinear). 

#### <br>

```{r}
library(faraway)
data(cheddar, package="faraway")

cheddar$H2S_OG<-exp(cheddar$H2S) # return H2S from log to original scale by exponentiating

set.seed(123)
X4_E<-rnorm(30,3,1) # creates error for X4 variable -- 30 draws from normal distribution with mean = 3 and sd = 1
summary(X4_E)
cheddar$X4<-10*cheddar$Lactic+X4_E # create X4 variable as function of Lactic and error so X4 and Lactic are collinear
cor(cheddar$X4,cheddar$Lactic) 

summary(cheddar)
```


### 1.B. Check collinearity 

* Consider the following three models:
  + \texttt{taste}~\texttt{Acetic+H2S\_OG+Lactic}
  + \texttt{taste}~\texttt{Acetic+H2S\_OG+X4}
  + \texttt{taste}~\texttt{Acetic+H2S\_OG+Lactic+X4}
  
* What do you conclude about collinearity of each model using eigenvalues, VIF and condition number and index?

```{r}
mod1<-lm(taste~Acetic+H2S_OG+Lactic,cheddar)
mod2<-lm(taste~Acetic+H2S_OG+X4,cheddar)
mod3<-lm(taste~Acetic+H2S_OG+Lactic+X4,cheddar)

sumary(mod1);sumary(mod2);sumary(mod3)
```
##### <br> 

* Eigenvalues 
```{r}
x_mod1<-model.matrix(mod1)[,-1] # want the model matrix, but without intercept as it's not helpful here
x_mod1
e1<-eigen(t(x_mod1)%*%x_mod1) # * is multiply in R %*% is matrix multiplication
e1
e1$val # Do you see a wide range in eigenvalues?
```
##### <br> 

* VIF
```{r}
vif(x_mod1) #vif() is from faraway 
```
##### <br> 

* Condition number and index
```{r}
kappa1<-sqrt(max(e1$val)/min(e1$val))
nu1<-sqrt(max(e1$val)/e1$val)
kappa1;nu1 
```
Are any of the condition numbers large?
Is K $\ge$ 30?

2 of the condition numbers are quite large, but the smallest is 1. Based on this and the VIF there doesn't appear to be problematic collinearity in model 1. 

##### <br> 
* For \texttt{mod2}
```{r}
x_mod2<-model.matrix(mod2)[,-1]
e2<-eigen(t(x_mod2)%*%x_mod2)
e2$val
vif(x_mod2) 

kappa2<-sqrt(max(e2$val)/min(e2$val))
nu2<-sqrt(max(e2$val)/e2$val)
kappa2;nu2
```
Again 2 of the condition numbers are quite large, but the smallest is 1. Based on this and the VIF there doesn't appear to be problematic collinearity in model 2. 

##### <br> 
* For \texttt{mod3}
```{r}
x_mod3<-model.matrix(mod3)[,-1]
e3<-eigen(t(x_mod3)%*%x_mod3)
e3$val
vif(x_mod3) 

kappa3<-sqrt(max(e3$val)/min(e3$val))
nu3<-sqrt(max(e3$val)/e3$val)
kappa3;nu3
```

```{r}
cor(cheddar[,c("Acetic","H2S_OG","Lactic","X4")])
```

* There is clear evidence of collinearity in mod3 with extremely large VIF on \texttt{Lactic} and \texttt{X4}.


### 1.C. Rescaling of \texttt{H2S\_OG} in mod1
```{r}
summary(cheddar$H2S_OG)
cheddar$H2S_N<-cheddar$H2S_OG/1000
cor(cheddar$H2S_OG,cheddar$H2S_N)

mod1<-lm(taste~Acetic+H2S_OG+Lactic,cheddar)
mod4<-lm(taste~Acetic+H2S_N+Lactic,cheddar)
mod5<-lm(taste~scale(Acetic)+scale(H2S_OG)+scale(Lactic),cheddar)

sumary(mod1);sumary(mod4);sumary(mod5)
```

```{r}
summary(scale(cheddar$Acetic))
summary(scale(cheddar$H2S_OG))
summary(scale(cheddar$H2S_N))
summary(scale(cheddar$Lactic))
```
What did the scale function do?

* By default it centers and scales the variable.

  
## 2. Checking assumptions about model errors 


### 2.A. We already know how to check zero mean, constant variance and normailty assumption from Class 8 lab session both graphically and numerically.


### 2.B. Focusing on the constant variance assumption

* We will intentionally create \texttt{taste2} by adding a nonconstant (i.e., heteroscedasticitic) error term, \texttt{E}, to \texttt{taste} and fit a model  \texttt{taste2}~\texttt{Acetic+H2S\_N+Lactic}.

```{r}
set.seed(521)
cheddar$E<-rnorm(30,0,sqrt(cheddar$H2S_OG))

cheddar$taste2<-mod4$coefficients[1]+
  mod4$coefficients[2]*cheddar$Acetic+
  mod4$coefficients[3]*cheddar$H2S_N+
  mod4$coefficients[4]*cheddar$Lactic+
  cheddar$E

summary(cheddar$taste);summary(cheddar$taste2)    
```

* First, check on the constant error variance assumption.
```{r}
mod6<-lm(taste2~Acetic+H2S_N+Lactic,cheddar)
summary(mod6)

plot(fitted(mod6),residuals(mod6))+
abline(h=0,col="red")
``` 

* The residual variance is becomes larger as $\hat{y}_i$ increases. This is a clear sign of heteroscedasticity (nonconstant variance). 

##### <br> 

* We can address heteroscedasticity with heteroscedasticity-consistent variance with \texttt{vcovHC()} as follows:
```{r}
library(dplyr)
library(sandwich)# For vcovHC()

hetvar <- mod6 %>% 
  vcovHC() %>% # calculate Heteroscedasticity-consistent estimation of the covariance matrix for coefficients
  diag() # gives variances as they are the diagonal of the covariance matrix


summary(mod6)

# Calculate 95% CI for Acetic using heteroscedasticity-consistent variance
mod6$coefficients[2]+c(-1,1)*qt(0.975,mod6$df.residual)*sqrt(hetvar[2])

# Construct 95% CI using heteroscedasticity-consistent variance for the other betas in mod6
```

* Replace standard errors with those from \texttt{vcovHC()} in the inference and hypothesis tests.

##### <br> 

* We can also address heteroscedasticity through weighted least square estimation as follows: 

```{r}
cheddar$abresid<-abs(residuals(mod6))
cheddar$yhat<-fitted.values(mod6)
```

```{r}
mod_yhat<-lm(abresid~yhat,cheddar)

cheddar$wt1<-1/(fitted.values(mod_yhat)*fitted.values(mod_yhat))
```

```{r}
plot(cheddar$Acetic,resid(mod6))
plot(cheddar$H2S_N,resid(mod6))
plot(cheddar$Lactic,resid(mod6))

cheddar$wt2<-1/cheddar$H2S_N
```

```{r}
mod_w1<-lm(taste2~Acetic+H2S_N+Lactic,cheddar,weights=wt1)
mod_w2<-lm(taste2~Acetic+H2S_N+Lactic,cheddar,weights=wt2)
summary(mod6)
summary(mod_w1)
summary(mod_w2)
```

##### <br>
* Examine the residuals of \texttt{mod6}, \texttt{mod\_w1} and \texttt{mod\_w2} graphically: 
```{r, fig.height=3, fig.width=4}
plot(fitted(mod6),resid(mod6)); abline(h=0,col="red")
plot(fitted(mod_w1),resid(mod_w1)); abline(h=0,col="red")
plot(fitted(mod_w2),resid(mod_w2)); abline(h=0,col="red")
```

* We see some improvement on the constant error variance assumption with WLS. 
 
### 1.D. Lack of Fit

* We will use flock.dat (Available on Canvas)
* Description: from R.D. Cook, and J.O. Jacobsen (1978) 
* "Analysis of 1977 West Hudson Bay Snow Goose Surveys" by Canadian Wildlife Service.
* Snow Geese Flock Size
* Traditionally, estimates of flock size were obtained by observation from small aircraft. An experiment was conducted to determine how accurate the observations of flock size were. In addition to observer's estimation of the size of 45 flocks of snow geese (OBS), a photograph was taken to record the exact flock size (PHOTO) for each flock (ID).
```{r}
flock <- read.delim("C:/Users/jkubale/University of Michigan Dropbox/John Kubale/survmeth685/data/flock.dat",sep="")
names(flock)
dim(flock)

plot(flock$PHOTO,flock$OBS,pch=18,col="blue",cex=.7,
     main="flock size by observation vs. photo (n=45)");abline(lm(OBS~PHOTO,flock),col="red")
```

```{r}
mod_OBS<-lm(OBS~PHOTO, flock)
mod_OBS_a<-lm(resid(mod_OBS)~factor(PHOTO),flock) # fits model with beta for every unique value of PHOTO

summary(mod_OBS)
summary(mod_OBS_a)


anova(mod_OBS)
# gives you total RSS: 81675
anova(mod_OBS_a)
# gives you SS Pure Error: 740
# SS Lack of Fit = 81675 - 740 = 80935

# get df pure error and df lack of fit
length(unique(flock$PHOTO))
# df lack of fit = # of unique x values - 2 = 37 - 2
# 2 in the equation above corresponds to the number of betas in the simple linear model
length(flock$PHOTO)
# df pure error = # of obs - # of unique x values = 45 - 37 = 8

# Calculate F statistic for lack of fit test
(((81675 - 740)/35)/(740/8))

# Calculate p-value for lack of fit test
1 - pf(24.99923,df1=35,df2=8)

## use the olsrr package to conduct a lack of fit test
library(olsrr)
ols_pure_error_anova(mod_OBS)
# will likely be some small differences between the F statistic and p-value calculated manually

```
* There is evidence for lack of fit in this data or model.
* Had the p-value been >0.05 this would not necessarily mean we have the correct model. It may be a limitation of the data.

Notice the difference in $R^2$ values between mod_OBS and mod_OBS_a. The $R^2$ for mod_OBS_a is nearly (but not quite!) 1. This makes sense as a parameter is included for each unique data point because (why we had factor(PHOTO) in the model). However, since there are repeated values for X in the data the $R^2$ can never actually be 1. Here it is quite close, but with other data it may not be. This shows another reason not to rely too heavily on statistics like $R^2$.
