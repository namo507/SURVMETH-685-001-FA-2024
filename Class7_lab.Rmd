---
title: "SMML Class 7 Lab"
author: "John Kubale"
date: "2024-10-08"
output: pdf_document
number_sections: yes
fontsize: 12pt
---


```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo=TRUE,cache=TRUE, 
                      autodep=TRUE, cache.comments=FALSE,
                      message=FALSE, warning=FALSE,
                      fig.width=4.5, fig.height=3)
```

  

```{r}
library(ISLR2)
library(dplyr)
library(gridExtra)
library(grid)
library(ggplot2)
```


##  <br> We'll start with the Wage data from the ISLR2 package. <br>

```{r}
data(Wage)
head(Wage)
```


### 1. What is the mean of \texttt{wage} by \texttt{jobclass}? What is the difference in the mean of \texttt{wage} by \texttt{jobclass}?
```{r}
Wage%>%
  group_by(jobclass)%>%
  summarise(m = mean(wage))%>%
  mutate(diff = m - lag(m, default = m[1])) 
## lag() function allows you to look at the previous observation 
## Default option allows you to specify what to use when an observation is first (i.e, there is no lag)
## Setting default to the first item (i.e., m[1]) tells R to subtract the industrial mean from itself giving 0
```
* The mean wage for the industrial jobclass is 103.32 and for the information jobclass is 120.6. The difference of wage is 17.3.

### 2. Fit a model with \texttt{wage} as a function of \texttt{jobclass} with and without an intercept. What do the coefficients mean? How are they related to the results from #1?
```{r}
mod_j<-lm(wage~jobclass,Wage)
summary(mod_j)
```
* The intercept is 103.32, corresponding to the mean wage of the industrial jobclass. The slope corresponds to the difference.

```{r}
mod_j_wo<-lm(wage~jobclass-1,Wage)
summary(mod_j_wo)
```
* The intercept is 103.32, corresponding to the mean wage of the industrial jobclass. The slope corresponds to the mean of the information jobclass.

* Recall the specification of a simple linear regression model with a binary predictor (p.# of the lecture notes):

$\begin{array}{r l}
y_i&=\mu_1d_{1i}+\mu_2d_{2i}+\epsilon_i\\[5pt]
&=\mu_1(1-d_{2i})+\mu_2d_{2i}+\epsilon_i\\[5pt]
&=\mu_1+(\mu_2-\mu_1)d_{2i}+\epsilon_i\\[5pt]
&=\gamma_0+\gamma_2d_{2i}+\epsilon_i\\[5pt] \end{array}$

* Hence, in the model from #3 (no intercept), $103.32=\hat{\mu}_{jobclass=1}$ and $120.59=\hat{\mu}_{jobclass=2}$.
* In the model from #2 (w intercept), $103.32=\hat{\mu}_{jobclass=1}$ and $17.27=\hat{\mu}_{jobclass=2}-\hat{\mu}_{jobclass=1}$.

### 3. Examine the ANOVA table of the two models from #2. What do you observe? 
```{r}
anova(mod_j)
anova(mod_j_wo)
# anova(mod_j, mod_j_wo)

```
* ANOVA results for the residuals are exactly the same. This is because the information given to the model to explain \texttt{wage} is not different between the two models.

### 4. Calculate the mean of \texttt{wage} by \texttt{race}; Fit a simple linear regression of \texttt{wage} as a function of \texttt{race}; Obtain ANOVA table of the regression model
```{r}
Wage%>%
  group_by(race)%>%
  summarise(m = mean(wage))
```
* The mean income varies: 
  - 112.56 for Whites; 
  - 101.60 for Blacks;
  - 120.29 for Asians; and 
  - 89.97 for Others.
\newpage

```{r}
mod_r<-lm(wage~race,Wage)
summary(mod_r)
```

* Recall with a categorical predictor with $f$ categories, consider $f-1$ dummies, $d_{2i},\cdots,d_{fi}$ (p. 18 of the lecture notes)\
$y_i=\gamma_0+\gamma_2d_{2i}+\cdots+\gamma_{f}d_{fi}+\epsilon_i$
* Category 1 (White) is the reference level.
* We can link the coefficients with the group means estimates as follows:
  - The intercept 112.56 is the mean of Whites. 
  - -10.96 is the difference between Blacks and Whites (=101.60-112.56); 
  - 7.72 (=120.29-112.56) is the difference between Asians and Whites; and 
  - -22.59(=89.97-112.56) is the difference between Others and Whites.
* Hence, $\gamma_0=\mu_{race=1}$, $\gamma_2=\mu_{race=2}-\mu_{race=1}$, etc.
* More broadly speaking, $\gamma_0=\mu_{race=ref category}$, $\gamma_j=\mu_{race=j}-\mu_{race=ref category}$, etc.

```{r}
anova(mod_r)
```
* The df of predictor is 3, which is 4 categories of \texttt{race} minus the reference category.


### 6. How about \texttt{year}? Try \texttt{year} as a continuous predictor as well as a categorical predictor in a regression model. Observe $F$ values and $df$'s.
```{r}
Wage%>%
  group_by(year)%>%
  summarise(m=mean(wage))
```
* There are a total of 7 different years in the data, with varying levels of mean wage. 

```{r}
class(Wage$year)
summary(lm(wage~year,Wage))
summary(lm(wage~factor(year),Wage))
```
* If year is specified as a continuous variable, the model df is 1. If specified as a categorical variable, the model df is 6. The $F$ statistics are vastly different, although both lead to rejecting the null hypothesis that none of the predictors have an effect on the outcome variable.


### 7. \texttt{wage} as a function of \texttt{jobclass} and \texttt{year} (first as a categorical variable and then a continous variable), and their interaction.

A. \texttt{year} as a categorical variable.

\small
```{r}
summary(lm(wage~jobclass*factor(year),Wage))
Wage%>%
  group_by(jobclass,year)%>%
  summarise(m=mean(wage))
```
\normalsize

* How are the coefficient estimates related to the group mean estimates?
  - The intercept coefficient estimate 98.93 is the same as $\mu_{Industrial;2003}$. We know this is the reference category of both categorical variables combined. 
  - The first slope coeff est is 16.08 is associated with jobclass="2. Information". Meaning, 16.08=$\mu_{Information;2003}-\mu_{Industrial;2003}$=115.01-98.93. 
  - The second slope coeff est, 5.822=$Industrial;2003-\mu_{Industrial;2003}$=104.73-98.93. 
  - The rest of the slope coeffs can be understood in the same fashion.
* Also see the plots on p. 52 of the lecture note

B. \texttt{year} as a continous variable.
```{r}
summary(lm(wage~jobclass*year,Wage))
```

$y_i=\gamma_0+\gamma_2d_{2i}+\gamma_3x_i+\gamma_4d_{2i}x_i+\epsilon_i$
* What are $\gamma_0$, etc.?

```{r, fig.width=7, fig.height=5}

year<-ggplot(aes(x=year,y=wage),data=Wage)+
  geom_smooth(method="lm", se=FALSE)

jobclass<-ggplot(aes(x=jobclass, y=wage, color=jobclass),data=Wage)+
  geom_violin(trim=T)+
  stat_summary(fun=mean, geom="point", size=2, color="red")+
  theme(legend.position="none")

int<-ggplot(aes(x=year,y=wage),data=Wage)+
  facet_grid(~jobclass)+
  geom_smooth(method="lm", se=FALSE) 

grid.arrange(year, jobclass, int, layout_matrix=rbind(c(1,2),c(3,3)))
```

* $\gamma_0$ is your intercept showing the mean expected wage for industrial workers in year 0 (might want to center!)
* $\gamma_2$ is the difference in mean expected wage in year 0 for information workers compared to industrial workers. Why might this be misleading?
* $\gamma_3$ is The expected change in expected mean wage with every year
* $\gamma_4$ is The rate at which the expected mean wage changes for information workers compared to industrial workers.



### 8. \texttt{wage} as a function of \texttt{year} (as a continuous predictor), \texttt{race} and their interaction. Also, try to set \texttt{race}="3. Asian" as the reference category.
\small
```{r}
summary(lm(wage~race*year,Wage))
```

```{r}
summary(lm(wage~year*relevel(race,ref="3. Asian"),Wage))
```
\normalsize

### 9. Visual examination of #8 

A. Violin plot of \texttt{wage} ~ \texttt{year} with the subgroup means displayed on the violins
```{r}
violin_y<-ggplot(aes(x=as.factor(year), y=wage, 
                     color=as.factor(year)), data=Wage)+
  geom_violin(trim=T)+
  stat_summary(fun=mean, geom="point", size=2, color="black")+
  theme(legend.position="none")
violin_y
```

B. Violin plot of \texttt{wage} ~ \texttt{race} with the subgroup medians displayed on the violins
```{r}
violin_r<-ggplot(aes(x=race, y=wage, color=race),data=Wage)+
  geom_violin(trim=T)+
  stat_summary(fun=median, geom="point", size=2, color="red")+
  theme(legend.position="none")
violin_r
```

C. Regression lines of \texttt{wage} ~ \texttt{year} by \texttt{race}
```{r}
int<-ggplot(aes(x=year,y=wage),data=Wage)+
  facet_grid(~race)+
  geom_smooth(method="lm", se=FALSE)
int
```

D. Boxplot of \texttt{wage} ~ \texttt{race} by \texttt{year} 
```{r, fig.width=6, fig.height=3.5}
ggplot(aes(x=race,y=wage, color=race),data=Wage)+
  facet_grid(~year)+
  geom_boxplot()+
   stat_summary(fun=mean, geom="point", size=2, color="black")+
   theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```

E. Putting plots into one
```{r, fig.width=7, fig.height=5}
layout <- rbind(c(1,2),
                c(3,3))

grid.arrange(violin_y, violin_r, 
             int, layout_matrix=layout)
```


 