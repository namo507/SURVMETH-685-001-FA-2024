---
title: "SMML Class 8 Lab"
author: "John Kubale"
date: "10/22/2024"
output: pdf_document
number_sections: yes
fontsize: 12pt
---


```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo=TRUE,cache=TRUE, 
                      autodep=TRUE, cache.comments=FALSE,
                      message=FALSE, warning=FALSE,
                      fig.width=4.5, fig.height=3)
```

  

```{r}
library(ISLR2)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(grid)
library(faraway)
```
 
##  <br> Use Wage data <br>
```{r}
data(Wage, package="ISLR2")
```
 
### 1. Fit 3 regression models using the Wage data from ISLR2. In model 1 fit a model with wage as a function of year. In model 2 fit a model with wage as a function of year and age. Finally, in model 3 fit a model with wage as a function of year, age, and jobclass. Interpret the coefficient for year in each model?

```{r}
sumary(lm(wage~year,Wage))
sumary(lm(wage~year+age,Wage))
sumary(lm(wage~year+age+jobclass,Wage))
```
* From \texttt{wage}~\texttt{year}, a 1-year increase equates to a 1.34 increase in workers' raw wage, and this effect is statistically significant (i.e., year has a significant effect on worker's wage).
* From \texttt{wage}~\texttt{year+age}, controlling for workers' age, a 1-year increase equates to a 1.19 increase in workers raw wage, and this effect is statistically significant (i.e., workers' age has a significant effect on wage); a 1-year increase in worker's age is associated with an  increase of 0.699 in wage, controlling for year, and this effect is statistically significant.
* From \texttt{wage}~\texttt{year+age+jobclass}, controlling for age and jobclass, a 1-year increase equates to a 1.23 increase in workers raw wage,  and this effect is statistically significant (i.e., year has a significant effect on wage); controlling for year and jobclass, a 1-year increase in worker's age is associated with an increase of 0.636 in wage, and this effect is statistically significant; controlling for year and worker's age, the wage of those in Information jobclass wage is 15.97 higher than the wage of those in Industrial jobclass,  and this job class effect is statistically significant.

### 2. Regression of \texttt{wage}~\texttt{year+age+jobclass} and \texttt{logwage}~\texttt{year+age+jobclass}

### 2.A. How do the coefficient estimates compare?
```{r}
sumary(lm(wage~year+age+jobclass,Wage))
sumary(lm(logwage~year+age+jobclass,Wage))
```
* Clearly, the coefficient estimates are different.

### 2.B. What do the coefficient estimates mean?
```{r}
coef(lm(wage~year+age+jobclass,Wage))
coef(lm(logwage~year+age+jobclass,Wage))
coef(lm(exp(logwage)~year+age+jobclass,Wage))
```


### 3. Check on the values of \texttt{year} and \texttt{age} variables.
```{r}
summary(Wage$year)
summary(Wage$age)
```

### 3.A. \texttt{year} in the data were from 2003 to 2009 and \texttt{age} from 18 to 80.  What would be our best guess for \texttt{wage} with the median value of \texttt{age} in 2015?

```{r}
mod1<-lm(wage~year+age, Wage)
summary(mod1)
mod1_x<-model.matrix(mod1) 
dim(mod1_x)
head(mod1_x)
# mod1_x is the matrix of predictors. In other words these are the data points being used to fit the model.

(mod1_x_star<-c(1,2015,apply(mod1_x,2,median)[3])) 
mod1_x_star2 <- data.frame("year" = 2015, "age" = median(Wage$age))

# This creates a vector with 1 (the intercept is always 1), year = 2015, and finds median of age which is the 3rd column in your model matrix, mod1_x.
test <- predict(mod1, newdata = mod1_x_star2, interval = "prediction")

(mod1_y_star<-sum(mod1_x_star*coef(mod1))) 
# this equates to 
# y_hat=beta0_hat+beta1_hat*median(year)+beta2_hat*median(age)

n <- length(Wage$wage)
y.fitted <- mod1$fitted.values
b0 <- mod1$coefficients[1]
b1 <- mod1$coefficients[2]
b2 <- mod1$coefficients[3]
mod_mat <- model.matrix(mod1)
x0 <- c(1, 2015, median(Wage$age))
# var(mod1)
pred.y <- (b0 + (b1*2015) + (b2*median(Wage$age)))
sse <- sum((Wage$wage - y.fitted)^2)
mse <- sse/(n-3)
t.val <- qt(0.975, n-3)

p.se <- sqrt(mse*(1+t(x0)%*%(t(mod_mat)%*%mod_mat)^-1%*%x0))

test <- predict(mod1, newdata = mod1_x_star2, interval = "predict")

pred.y - (t.val*p.se)
pred.y + (t.val*p.se)

## another way to do this that's a bit more straight-forward
datnew <- data.frame("year" = 2015, "age" = median(Wage$age))

predictions <- predict(
  mod1,
  newdata = datnew,
  type='response',
  se=T
)

quant <- qt(0.025, nrow(Wage)-ncol(mod1_x), lower.tail = F)

# Confidence interval -- here for your reference
# df_preds <- data.frame(datnew, predictions)%>%
#   mutate(lower = fit - (quant*se.fit),
#          upper = fit + (quant*se.fit))%>%
#   summarise(mn_beta = mean(fit),
#             lcl = mean(lower),
#             ucl = mean(upper))
# 
# print(df_preds)
# predict(mod1, newdata = datnew, interval = "confidence")

# Prediction interval
sigma2 <- sum(residuals(mod1)^2)/(nrow(Wage) - 3) ## AKA MSE
se.fit.pred <- sqrt(sigma2*(1+(predictions$se.fit^2/sigma2)))

df_preds_pred <- data.frame(datnew, predictions)%>%
  mutate(lower = fit - (quant*se.fit.pred),
         upper = fit + (quant*se.fit.pred))%>%
  summarise(mn_beta = mean(fit),
            lcl = mean(lower),
            ucl = mean(upper))

print(df_preds_pred)  
predict(mod1, newdata = datnew, interval = "prediction")
```



### 3.B. What is its 95% confidence interval?

```{r}
(mod1_x_star_dat<-data.frame(t(mod1_x_star)))
# t() is the transpose function -- e.g., will change vector that is 1 row with n columns to 1 column with n rows
colnames(mod1_x_star_dat)<-c("(Intercept)","year","age")
# this makes the column name of mod1_x_star_dat to be identical to lm output

mod1_x_star_dat

predict(mod1, new=mod1_x_star_dat, interval="confidence") 
```
* The 95% CI is: [`r round(predict(mod1, new=mod1_x_star_dat, interval="confidence")[2],3)`, `r round(predict(mod1, new=mod1_x_star_dat, interval="confidence")[3],3)`]

### 3.C. How about its 95% prediction interval? How would you evaluate its credibility?
```{r}
predict(mod1, new=mod1_x_star_dat, interval="prediction")
```
* The 95% prediction interval is: [`r round(predict(mod1, new=mod1_x_star_dat, interval="prediction")[2],3)`, `r round(predict(mod1, new=mod1_x_star_dat, interval="prediction")[3],3)`]. 
* The interval is quite large in that, for the next new person at the median age in 2015, the wage can be anywhere between 42.03 to 202.85 with 95% confidence. The predicted value 122.44 may not be of high credibility.

### 3.D. What do you observe between the confidence vs. prediction intervals? Why are there differences? 
```{r}
summary(mod1)
anova(mod1)
```
* The 95% confidence interval is: [`r round(predict(mod1, new=mod1_x_star_dat, interval="confidence")[2],3)`, `r round(predict(mod1, new=mod1_x_star_dat, interval="confidence")[3],3)`]. 
* The 95% prediction interval is: [`r round(predict(mod1, new=mod1_x_star_dat, interval="prediction")[2],3)`, `r round(predict(mod1, new=mod1_x_star_dat, interval="prediction")[3],3)`]. 
* The prediction interval is much wider than the CI. This is because the prediction interval considers the model error. As shown in the output, this model is not really good at explaining  wage, with a large RSS. This contributes to the wide prediction interval. Note that the CI does not consider the model error.


### 4. What if we want to predict \texttt{wage} for a 50 year old working in information jobclass in 2015?

### 4.A. Does the approach below work? Why or why not?
```{r, error=TRUE}
mod2<-lm(wage~year+age+jobclass, Wage)
mod2_x<-model.matrix(mod2)
mod2_x[1:5,]

(mod2_x_new<-t(c(1,2015, 50, 1)))
colnames(mod2_x_new)<-c("(Intercept)","year","age","jobclass2. Information")
mod2_x_new

predict(mod2, new=data.frame(mod2_x_new))
predict(mod2, new=data.frame(mod2_x_new), interval="confidence")
predict(mod2, new=data.frame(mod2_x_new), interval="prediction")
table(Wage$jobclass) ## how the raw data appear (how predict looks for the data)
table(mod2_x[,4]) ## how jobclass appears in model matrix (how R fits the model)

```
* This does not work -- the issue of categorical variables. \texttt{lm()} in R automatically creates dummies for categorical variable in the background. It's trying to create dummy variables of a dummy variable which is why it's failing. 

### 4.B. What to do?

* Fit the model with dummies for categorical variables!
* E.g., for a categorical variable with $f$ categories, need $f-1$ dummies.

```{r}
Wage$information<-with(Wage,ifelse(jobclass=="2. Information",1,0))
# with() tells R which data or object you want the subsequent function to be applied to
# ifelse() follows the format (T/F condition to test, value if condition true, value if condition false)
# so this code creates a new variable in Wage called "information" that == 1 when jobclass == "2. Information" and 0 otherwise

mod3<-lm(wage~year+age+information, Wage)
mod3_x<-model.matrix(mod3)
mod3_x[1:5,]

(mod3_x_new<-t(c(1,2015, 50, 1)))
colnames(mod3_x_new)<-c("(Intercept)","year","age","information")
mod3_x_new

predict(mod3, new=data.frame(mod3_x_new))
predict(mod3, new=data.frame(mod3_x_new), interval="confidence")
predict(mod3, new=data.frame(mod3_x_new), interval="prediction")
```


### 5. Consider fitted values from simple regression ($\hat{y}_{simple}$) vs. multiple regression ($\hat{y}_{multiple}$) vs. observed values ($y$). 

```{r}
mod_simple<-lm(wage~age,Wage)
mod_multiple<-lm(wage~age+year*race+maritl+education+health,Wage)

Wage$wage_hat_simple<-fitted(mod_simple)
Wage$wage_hat_multiple<-fitted(mod_multiple)
```

### 5.A. Examine their distribution through \texttt{summary()}.
```{r}
summary(Wage$wage)
summary(Wage$wage_hat_simple)
summary(Wage$wage_hat_multiple)
```

### 5.B. Compare the two types of fitted values against the observed through correlation coefficients and scatterplots. 
```{r}
cor(Wage$wage,Wage$wage_hat_simple)
cor(Wage$wage,Wage$wage_hat_multiple)
```

```{r, fig.width=7, fig.height=3.5}
simple<-ggplot(Wage,aes(x=wage,y=wage_hat_simple))+
  geom_point(color="blue", size=0.5)+
  coord_cartesian(xlim = c(0, 320), ylim = c(0, 320)) 
multiple<-ggplot(Wage,aes(x=wage,y=wage_hat_multiple))+
  geom_point(color="red", size=0.5)+
  scale_x_continuous(limits = c(20, 320))+
  scale_y_continuous(limits = c(20, 320))

grid.arrange(simple,multiple, ncol=2)
```

### 5.C. Which model is better? Why?
```{r}
summary(mod_simple)$adj.r.square;summary(mod_multiple)$adj.r.square
deviance(mod_simple);deviance(mod_multiple) # AKA RSS
anova(mod_simple, mod_multiple)
```
* Multiple linear regression model is better from all three tests.
 
