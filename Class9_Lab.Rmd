---
title: "SMML Class 9 Lab"
author: "John Kubale"
date: "10/29/2024"
output:  pdf_document
number_sections: yes
fontsize: 12pt
---


```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo=TRUE,cache=TRUE, 
                      autodep=TRUE, cache.comments=FALSE,
                      message=FALSE, warning=FALSE,
                      fig.width=4.5, fig.height=3)
```

  
  
## 1. Checking assumptions about errors through residuals  

### 1.A. Let's use cheddar data from faraway

* Description: In a study of cheddar cheese from the LaTrobe Valley of Victoria, Australia, samples of cheese were analyzed for their chemical composition and were subjected to taste tests. Overall taste scores were obtained by combining the scores from several tasters.

* Format: A data frame with 30 observations on the following 4 variables.
  + \texttt{taste}: a subjective taste score
  + \texttt{Acetic}: concentration of acetic acid (log scale)
  + \texttt{H2S}: concentration of hydrogen sulfice (log scale)
  + \texttt{Lactic}: concentration of lactic acid
   


```{r}
library(faraway)
data(cheddar)
summary(cheddar)
```

* Fit a linear regression model with taste as a function of Acetic, H2S, and Lactic.
```{r}
mod_cheddar<-lm(taste~Acetic+H2S+Lactic,cheddar)
summary(mod_cheddar)
```

### 1.B. Brief check on the distribution of residuals, $\hat{\epsilon}$.

* Think about mean, constant variance and normal distribution
```{r}
summary(resid(mod_cheddar))
mean(resid(mod_cheddar))
```
```{r,fig.width=4, fig.height=4}
hist(resid(mod_cheddar))  
plot(density(resid(mod_cheddar)),
     main="Density Plot")
```
 
* Graphic: Zero mean and constant variance
```{r,fig.width=4, fig.height=4}
plot(fitted(mod_cheddar),resid(mod_cheddar))+ 
abline(h=0,col="blue")
```


* Numeric: Constant variance
```{r}
cor(fitted(mod_cheddar),resid(mod_cheddar))
summary(lm(resid(mod_cheddar)~fitted(mod_cheddar)))
var.test(resid(mod_cheddar)[fitted(mod_cheddar)<=20],
         resid(mod_cheddar)[fitted(mod_cheddar)>20])
```
* No reason to believe that the residual is related to $\hat{y}$

* How is the graphical approach useful? How about numeric/statistical approaches?
```{r,fig.width=4, fig.height=4}
plot(fitted(mod_cheddar),sqrt(abs(resid(mod_cheddar))))
cor(fitted(mod_cheddar),sqrt(abs(resid(mod_cheddar))))

plot(cheddar$Acetic,resid(mod_cheddar))+
abline(h=0,col="blue")
cor(cheddar$Acetic,resid(mod_cheddar))

plot(cheddar$H2S,resid(mod_cheddar))+
abline(h=0,col="blue")
cor(cheddar$H2S,resid(mod_cheddar))

plot(cheddar$Lactic,resid(mod_cheddar))+
abline(h=0,col="blue")
cor(cheddar$Lactic,resid(mod_cheddar))
```


* Normal distribution
```{r,fig.width=4, fig.height=4}
qqnorm(resid(mod_cheddar))
shapiro.test(resid(mod_cheddar))
```

* What do you conclude about the residuals?
  + No clear reason to believe that the assumptions of the zero mean, the constant variance and the normal distribution are violated.
* How about inferences based on the output of \texttt{summary(mod\_cheddar)}?
  + Given that the assumptions are met, respective coefficient estimates follow a normal distribution. Hence, inferences using these estimates are valid.



## 2. Use \texttt{perfect\_add5} data and examine the existence of unusual observations

```{r}
id<-c(1:100)
set.seed(312)
x<-rnorm(100,15,5)
set.seed(135)
e<-rnorm(100,0,4)
y<-10+3*x+e

perfect<-as.data.frame(cbind(id,y,x))

d<-c("id","y","x")

add5<-as.data.frame(setNames(list(c(181:200),
                                  c(sample(c(45:60),20,rep=T)),
                                  c(sample(c(15:20),20,rep=T))),d))
perfect_add5<-rbind(perfect,add5)
```

### 2.A. Leverage through hat (and half normal plots) and standardized (i.e., internally studentized) residuals 

* hat and halfnormal plot
```{r}
mod_perfect_add5<-lm(y~x,perfect_add5)
hat<-hatvalues(mod_perfect_add5)
hat
sum(hat) # sum of hat values should equal the number of parameters in model
mean(hat)

hat>2*mean(hat)
```

```{r,fig.width=4, fig.height=4}
halfnorm(hat) #halfnorm() from faraway package
```


```{r}
perfect_add5[32,]
perfect_add5[107,]
summary(perfect_add5)
```


* Standardized residuals (i.e., internally studentized residuals)
```{r}
r_it<-rstandard(mod_perfect_add5)
```

```{r,fig.width=4, fig.height=4}
qqnorm(r_it)+
abline(0,1,col="red")
```



### 2.B. Outlier detection through (externally) studentized residuals
```{r}
summary(mod_perfect_add5)
r_ex<-rstudent(mod_perfect_add5)
abs(sort(r_ex)[1:10])
abs(sort(r_ex)[1:10])>abs(qt(.05/(10),120-2))

abs(r_ex)[111]
```


### 2.C. Influential observations through Cook's distance
```{r}
cd<-cooks.distance(mod_perfect_add5)
summary(cd)
sort(cd,dec=T)[1:10]
```

```{r,fig.width=4, fig.height=4}
halfnorm(cd)
```

```{r}
perfect_add5[c(32,107,111,113,114),]
summary(perfect_add5)
```

```{r,fig.width=3.5, fig.height=3.5}
plot(mod_perfect_add5)
```

### 2.D. Given results from #2.A to #2.C, what do we conclude about the influential observations in \texttt{perfect\_add5}? 
* It appears obs 32, 107, 113 and 114 are influential in a way that they may be altering the relationship of the majority of observations. (This makes sense as obs 101-120 are added apart from the implied relationship of obs 1-100.) This warrants examining the relationship without these observations.

```{r,fig.width=4, fig.height=4}
plot(perfect_add5$x,perfect_add5$y)+
abline(lm(y~x,perfect),col="red")+
abline(lm(y~x,perfect_add5),col="blue")
```

## 3. Checking relationship structures through partial regression plots and partial residual plots using \texttt{cheddar} data

### 3.A. Partial regression:

```{r}
rd_taste<-residuals(lm(taste~Acetic+Lactic,cheddar)) 
rd_H2S<-residuals(lm(H2S~Acetic+Lactic,cheddar)) 
```
* What would \texttt{rd\_taste} mean? 
  + \texttt{taste} without the effects of \texttt{Acetic} and \texttt{Lactic}
* What would \texttt{rd\_H2S} mean?
  + \texttt{H2S} without the effects of \texttt{Acetic} and \texttt{Lactic}


```{r,fig.width=4, fig.height=4}
plot(rd_H2S,rd_taste)+
abline(0,coef(mod_cheddar)['H2S'], col="red")  
```

* Is the relationship between \texttt{rd\_taste} and \texttt{rd\_H2S} linear?
  + Roughly speaking, yes. 
  + This also means that \texttt{taste} and \texttt{H2S} have a linear relationship.


### 3.B. Partial Residuals
```{r,fig.width=4, fig.height=4}
termplot(mod_cheddar, partial.resid=T, terms=1)
termplot(mod_cheddar, partial.resid=T, terms=2)
termplot(mod_cheddar, partial.resid=T, terms=3)
# termplot() is a function that plots regression terms against their predictors
```


### 3.C. Regression on subsetted data
```{r}
summary(cheddar)
mod1<-lm(taste~Acetic+H2S+Lactic,subset(cheddar,H2S<=5.3))
mod2<-lm(taste~Acetic+H2S+Lactic,subset(cheddar,H2S>5.3))
sumary(mod1); sumary(mod2)
confint(mod1); confint(mod2) 
```

* What do you observe in the estimated coefficients of \texttt{H2S} in these models?
  + We can't reject the hypothesis of the coefficient being different from zero in both models. Hence, there is no reason to believe that the relationship between \texttt{taste} and \texttt{H2S} differs by the ranges of \texttt{H2S}.
 