---
title: "Homework 7_HW3 17_Yael Dejene Beshaw & Namit Shrivastava"
format: pdf
editor: visual
---

```{r}
# Firstly loading the necessary libraries
library(faraway)
library(ggplot2)
library(car)

# Then we load the teengamb dataset
data(teengamb)

# Fitting the linear regression model now
modelgamb <- lm(gamble ~ sex + status + income + verbal, data = teengamb) 
# Summary of the model 
summary(modelgamb)
```
# 1.A. Check the zero mean error assumption using residual plots

```{r}
residuals <- resid(modelgamb)
fitted <- fitted(modelgamb)
ggplot(data.frame(fitted, residuals), aes(x = fitted, y = residuals)) +
    geom_point() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")
```
So, the residual plot shows the residuals (errors) on the y-axis and the fitted values on the x-axis.
Now from class theory we know that the zero mean error assumption is met if the residuals are randomly scattered around the horizontal line at zero. So from the scatter plot, we can see that there is no pattern like funnel or curvature, hence we can say that the model's errors have a mean of zero, and the assumption is met.


# 1.B. Check the constant error variance assumption using both residual plots and a formal statistical test
```{r}
#Two ways to check the residual plots
ggplot(data.frame(fitted, residuals), aes(x = fitted, y = abs(residuals))) +
    geom_point() +
    geom_smooth(method = "loess") +
    labs(title = "Scale-Location Plot", x = "Fitted Values",
    y = "Absolute Residuals")

#And then using simply this

plot(modelgamb, which = 1)

# Then, performing Breusch-Pagan test
#as the formal statistical test
library(lmtest)
bptest(modelgamb)
```
So the plot is a Scale-Location plot where points are randomly scattered around the horizontal line and since there is no clear pattern, it suggests that the constant error variance assumption is met.

Now talking about the BP test's p value which comes out to be 0.1693, which is not significant at the 0.05 level. This suggests that there is no significant evidence of heteroscedasticity, and the constant error variance assumption is likely met.


# 1.C. Check the error normality assumption both graphically and statistically
```{r}
qnorm(residuals)
qqPlot(modelgamb, main = "Q-Q Plot")
```
So we observe that the points in the Q-Q plot roughly follow the diagonal line and hence it means that the residuals are normally distributed.

Also in the plot, point 24 and 39 are seen to be potential outliers which may affect the normality of the residuals.

# Perform Shapiro-Wilk test
```{r}
shapiro.test(residuals)
```
So for this test we have the null hypothesis as; residuals are normally distributed and alternate hypothesis being that the residuals are not normally distributed.

Now based on the result obtained, we see that the p-value = 8.16e-05 means it is less than 0.05, and so we reject the null hypothesis. This actually suggests that there is a significant evidence of deviation from normality.


# 1.D. Check for observations with large leverage
```{r}
leverage <- hatvalues(modelgamb)
plot(leverage, type = "h", main = "Leverage Values", ylab = "Leverage")
```

So from class's theory, we know that the leverage values measure the influence of each observation on the fitted values.

Now from the graphs, the observations like index 35 and 42 have high leverage and should be investigated further to understand their influence on the model because later on we need to consider whether these points are valid data points or if they need to be addressed as outliers or errors.

So by identifying these high leverage points, i think we can actually ensure that the regression model is not unduly influenced by a few observations and improve the model's reliability and robustness.


# 1.E. Check for outliers
```{r}
studentized_residuals <- rstudent(modelgamb)
plot(studentized_residuals, type = "h", main = "Studentized Residuals", ylab = "Studentized Residuals")
```


# 1.F. Check for influential points
```{r}
cooksd <- cooks.distance(modelgamb)
plot(cooksd, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")
```



# 1.G. Check the structure of relationship between the predictors and the response
```{r}
pairs(teengamb, main = "Scatterplot Matrix")
```
